{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install wandb\n!pip install torchaudio\n!pip install editdistance==0.3.1","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.10.4)\nRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.13.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.7.0)\nRequirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.5.4)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.23.0)\nRequirement already satisfied: sentry-sdk>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.18.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.0.0)\nRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (7.1.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (5.3.1)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.1)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.14.0)\nRequirement already satisfied: watchdog>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.10.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.12.0->wandb) (46.1.3.post20200325)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2020.6.20)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.4)\nRequirement already satisfied: pathtools>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from watchdog>=0.8.3->wandb) (0.1.2)\nRequirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.2)\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (0.6.0a0+f17ae39)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchaudio) (1.6.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchaudio) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->torchaudio) (1.18.5)\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting editdistance==0.3.1\n  Downloading editdistance-0.3.1.tar.gz (19 kB)\nBuilding wheels for collected packages: editdistance\n  Building wheel for editdistance (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for editdistance: filename=editdistance-0.3.1-cp37-cp37m-linux_x86_64.whl size=212630 sha256=9eca6eb881dc4936347806cf5be2c916efb539cf7f1eab773c4721941924639c\n  Stored in directory: /root/.cache/pip/wheels/a9/9e/6f/0c07a94bbfae707c540b9cd2d7be284e0bc02ecd1234a3b6ed\nSuccessfully built editdistance\nInstalling collected packages: editdistance\nSuccessfully installed editdistance-0.3.1\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torch_optimizer","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting torch_optimizer\n  Downloading torch_optimizer-0.0.1a16-py3-none-any.whl (51 kB)\n\u001b[K     |████████████████████████████████| 51 kB 288 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from torch_optimizer) (1.6.0)\nCollecting pytorch-ranger>=0.1.1\n  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.1.0->torch_optimizer) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\nInstalling collected packages: pytorch-ranger, torch-optimizer\nSuccessfully installed pytorch-ranger-0.1.1 torch-optimizer-0.0.1a16\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wandb\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchaudio\nimport pandas as pd\nimport os\nfrom torch.optim import Adam\nfrom torch.utils.data import Subset, Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport numpy as np\nimport torch_optimizer\nfrom torch.optim.lr_scheduler import CosineAnnealingLR","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nSR = 16000\nN_MELS = 80\nAUDIO_LEN = 365472\n\nTRAIN_DS = 'cv-valid-train.csv'\nDEV_DS = 'cv-valid-dev.csv'\n\nCHAR_VOCAB = {k: v for v, k in enumerate(['<>'] + list(string.ascii_lowercase) + [' '])}\nTOK_VOCAB = {k:v for k, v in enumerate([''] + list(string.ascii_lowercase) + [' '])}\nALPHABET = np.array([''] + list(string.ascii_lowercase) + [' '])\n#'<>' means blank","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, checkpoint, patience=7, verbose=False, delta=0, min_loss=np.inf):\n        \"\"\"\n        :param\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None if min_loss == np.inf else  -min_loss\n        self.early_stop = False\n        self.val_loss_min = min_loss\n        self.delta = delta\n        self.checkpoint = checkpoint\n\n    def __call__(self, val_loss, model, epoch):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, epoch)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, epoch)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, epoch):\n        \"\"\"\n        Saves model when validation loss decrease.\n        \"\"\"\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'loss': val_loss\n            }, self.checkpoint)\n\n        self.val_loss_min = val_loss","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TSC(nn.Module):\n    def __init__(self, kernel_size, in_channels, out_channels, n_groups=1, \n                 dilation=1, stride=1):\n        super(TSC, self).__init__()\n        self.tsc = nn.Sequential(\n            nn.Conv1d(in_channels, in_channels, kernel_size, \n                      dilation=dilation, stride=stride,\n                      groups=in_channels, padding=dilation * kernel_size // 2),\n            nn.Conv1d(in_channels, out_channels, 1, groups=n_groups),\n            nn.BatchNorm1d(out_channels)\n        )\n\n    def forward(self, x):\n        x = self.tsc(x)\n        return x  \n\n\nclass TSCActivated(nn.Module):\n    def __init__(self, kernel_size, in_channels, out_channels, n_groups=1, \n                 dilation=1, stride=1):\n        super(TSCActivated, self).__init__()\n        self.tsc = TSC(kernel_size, in_channels, out_channels, n_groups, \n                       dilation, stride)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        x = self.tsc(x)\n        x = self.activation(x)\n        return x  \n\n\nclass TSCBlock(nn.Module):\n    def __init__(self, n_blocks, kernel_size, in_channels, out_channels,\n                 n_groups=1, is_intermediate=False):\n        super(TSCBlock, self).__init__()\n        if is_intermediate:\n            in_channels = out_channels\n        self.n_blocks = n_blocks\n        self.tsc_list = nn.ModuleList([TSCActivated(kernel_size, in_channels, out_channels, n_groups)])\n        self.tsc_list.extend([TSCActivated(kernel_size, out_channels, out_channels, n_groups) \n                                  for i in range(1, self.n_blocks-1)])\n        self.tsc_list.append(TSC(kernel_size, out_channels, out_channels, n_groups))\n        self.pnt_wise_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, groups=n_groups)\n        self.bn = nn.BatchNorm1d(out_channels)\n        self.relu = nn.ReLU(True)\n\n    def forward(self, x):\n        x_res = self.bn(self.pnt_wise_conv(x))\n        for layer in self.tsc_list:\n            x = layer(x)\n        return self.relu(x + x_res)\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, kernel_size, in_channels, out_channels, dilation=1, stride=1):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(in_channels, out_channels, kernel_size, \n                      padding=kernel_size // 2, dilation=dilation, \n                      stride=stride),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass Debug(nn.Module):\n    def __init__(self, msg=''):\n        super().__init__()\n        self.msg = msg\n    \n    def forward(self, x):\n        print(f'{x.shape}\\n{self.msg}')\n        return x\n\n\nclass QuarzNet(nn.Module):\n    def __init__(self, config):\n        super(QuarzNet, self).__init__() \n        self.config = config\n        self.s = config['s']\n        self.net = nn.Sequential(\n            TSCActivated(**config['c1']),\n            *[TSCBlock(**config['b1'], is_intermediate=bool(i)) for i in range(s)],\n            *[TSCBlock(**config['b2'], is_intermediate=bool(i)) for i in range(s)],\n            *[TSCBlock(**config['b3'], is_intermediate=bool(i)) for i in range(s)],\n            *[TSCBlock(**config['b4'], is_intermediate=bool(i)) for i in range(s)],\n            *[TSCBlock(**config['b5'], is_intermediate=bool(i)) for i in range(s)],\n            TSCActivated(**config['c2']),\n            TSCActivated(**config['c3']),\n            nn.Conv1d(**config['c4']),\n            nn.LogSoftmax(dim=1),\n        )\n\n    def forward(self, x):\n        x = self.net(x)\n        return x","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_param_dict(names, params):\n    param_dict = {n : p for n, p in zip(names, params)}\n    return param_dict\n\nn_labels = len(CHAR_VOCAB)\n\nc_names = ['kernel_size', 'in_channels', 'out_channels', 'dilation', 'stride']\nc1, c2 = [33, N_MELS, 256, 1, 2], [87, 512, 512, 2, 1]\nc3, c4 = [1, 512, 1024, 1, 1], [1, 1024, n_labels, 1, 1]\n\nb_names = ['n_blocks', 'kernel_size', 'in_channels', 'out_channels', 'n_groups']\nn_groups = 1\nb1, b2 = [5, 33, 256, 256, n_groups], [5, 39, 256, 256, n_groups]\nb3, b4 = [5, 51, 256, 512, n_groups], [5, 63, 512, 512, n_groups]\nb5 = [5, 75, 512, 512, n_groups]\ns = 1\n\n\n\nconfig = {\n    'c1': make_param_dict(c_names, c1),\n    'b1': make_param_dict(b_names, b1),\n    'b2': make_param_dict(b_names, b2),\n    'b3': make_param_dict(b_names, b3),\n    'b4': make_param_dict(b_names, b4),\n    'b5': make_param_dict(b_names, b5),\n    'c2': make_param_dict(c_names, c2),\n    'c3': make_param_dict(c_names, c3),\n    'c4': make_param_dict(c_names, c4),\n    's' : s\n}","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CharRNN(nn.Module):\n    \n    def __init__(self, tokens, n_hidden=612, n_layers=4,\n                               drop_prob=0.5, lr=0.001):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.lr = lr\n        self.chars = tokens\n        self.int2char = dict(enumerate(self.chars))\n        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(n_hidden, len(self.chars))\n      \n    \n    def forward(self, x, hidden):\n        ''' Forward pass through the network. \n            These inputs are x, and the hidden/cell state `hidden`. '''\n                \n        r_output, hidden = self.lstm(x, hidden)\n        out = self.dropout(r_output)\n        out = out.contiguous().view(-1, self.n_hidden)\n        out = self.fc(out)\n        \n       \n        \n        # return the final output and the hidden state\n        return out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n        \n        return hidden\n\n    \ndef one_hot_encode(arr, n_labels):\n    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n    one_hot = one_hot.reshape((*arr.shape, n_labels))\n    \n    return one_hot","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nchars = tuple([\"'\"] + list(string.ascii_lowercase) + [' '])\n\nwith open('../input/simple-lm/rnn_25_epoch.pt', 'rb') as f:\n    checkpoint = torch.load(f)\n\nloaded = CharRNN(chars, n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\nloaded.load_state_dict(checkpoint['state_dict'])\nloaded.to(device)\n\nprint(loaded)","execution_count":11,"outputs":[{"output_type":"stream","text":"CharRNN(\n  (lstm): LSTM(28, 512, num_layers=4, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=512, out_features=28, bias=True)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(net, char, h=None, top_k=None):\n        ''' Given a character, predict the next character.\n            Returns the predicted character and the hidden state.\n        '''\n        x = np.array([[net.char2int[char]]])\n        x = one_hot_encode(x, len(net.chars))\n        inputs = torch.from_numpy(x)\n        \n        inputs = inputs.to(device)\n        h = tuple([each.data for each in h])\n        out, h = net(inputs, h)\n        p = F.softmax(out, dim=1).data\n        p = p.cpu() \n        return p, h","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lm_prob(net, text):\n    if net is None:\n        return 0.0\n    net.eval()\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    prob = 1.0\n    text += \"'\"\n    h = None\n    for in_c, out_c in zip(text[:-1], text[1:]):\n        x = np.array([[net.char2int[in_c]]])\n        x = one_hot_encode(x, len(net.chars))\n        inputs = torch.from_numpy(x)\n\n        inputs = inputs.to(device)\n        out, h = net(inputs, h)\n        p = (F.softmax(out, dim=1).data).squeeze()\n        prob *= p[net.char2int[out_c]].cpu().item() \n    return prob","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_lm_prob(loaded, 'hello'))\nprint(get_lm_prob(loaded, 'hella'))","execution_count":14,"outputs":[{"output_type":"stream","text":"1.0455580190219525e-08\n6.207382350092737e-10\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = QuarzNet(config)\n\nprint(\"Total number of trainable parameters:\", count_parameters(model))\n\nfor module in model.net:\n    print(count_parameters(module))","execution_count":15,"outputs":[{"output_type":"stream","text":"Total number of trainable parameters: 6742460\n23968\n441344\n449024\n1439744\n1745920\n1776640\n308736\n528384\n28700\n0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.lower()\n    return text\n\ndef measure_len(root, folder, filename):\n    audio_file = os.path.join(os.path.join(root, folder), filename)\n    file, sr = torchaudio.load_wav(audio_file)\n    return file.shape[1]\n\ndef preprocess_targets(root, folder, lblpath):\n    target_file = os.path.join(root, lblpath)\n    targets = pd.read_csv(target_file)\n    targets = targets.dropna(subset=['text'])\n    new_targets = pd.DataFrame({'filename' : targets['filename'].values})\n    new_targets['cleaned_text'] = targets['text'].apply(clean)\n    new_targets['audio_len'] = targets['filename'].apply(lambda x: measure_len(root, folder, x))\n    new_targets = new_targets[new_targets['audio_len'] <= AUDIO_LEN]\n    new_targets.to_csv(lblpath, index=False)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndev_folder = 'cv-valid-dev'\npreprocess_targets('../input/common-voice/', dev_folder, DEV_DS)\ntrain_folder = 'cv-valid-train'\npreprocess_targets('../input/common-voice/', train_folder, TRAIN_DS)\n'''","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"\"\\ndev_folder = 'cv-valid-dev'\\npreprocess_targets('../input/common-voice/', dev_folder, DEV_DS)\\ntrain_folder = 'cv-valid-train'\\npreprocess_targets('../input/common-voice/', train_folder, TRAIN_DS)\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n#audio_len = targets['audio_len'].quantile(0.95)\ntargets = pd.read_csv('cv-valid-dev.csv')\nprint(len(targets.index))\ntargets.head()\n#targets = targets[targets['audio_len'] < AUDIO_LEN]\ntargets = targets[targets['cleaned_text'] != 'undefined']\ntargets.to_csv('cv-valid-dev.csv')\n\ntargets = pd.read_csv('cv-valid-train.csv')\nprint(len(targets.index))\ntargets.head()\n#targets = targets[targets['audio_len'] < AUDIO_LEN]\ntargets = targets[targets['cleaned_text'] != 'undefined']\ntargets.to_csv('cv-valid-train.csv')\n'''","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"\"\\n#audio_len = targets['audio_len'].quantile(0.95)\\ntargets = pd.read_csv('cv-valid-dev.csv')\\nprint(len(targets.index))\\ntargets.head()\\n#targets = targets[targets['audio_len'] < AUDIO_LEN]\\ntargets = targets[targets['cleaned_text'] != 'undefined']\\ntargets.to_csv('cv-valid-dev.csv')\\n\\ntargets = pd.read_csv('cv-valid-train.csv')\\nprint(len(targets.index))\\ntargets.head()\\n#targets = targets[targets['audio_len'] < AUDIO_LEN]\\ntargets = targets[targets['cleaned_text'] != 'undefined']\\ntargets.to_csv('cv-valid-train.csv')\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torchaudio\n\nfrom torchvision.transforms import Compose\n\n#https://github.com/toshiks/number_recognizer/blob/master/app/dataset/preprocessing.py\n\nclass LogMelSpectrogram(nn.Module):\n    \"\"\"\n    Create spectrogram from raw audio and make\n    that logarithmic for avoiding inf values.\n    \"\"\"\n    def __init__(self, sample_rate: int = 16000, n_mels: int = 128):\n        super(LogMelSpectrogram, self).__init__()\n        self.transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels,\n                                                              n_fft=1024, hop_length=256,\n                                                              f_min=0, f_max=8000)\n\n    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n        spectrogram = self.transform(waveform)\n        log_mel = torch.log(spectrogram + 1e-9)\n        return (log_mel - log_mel.mean()) / (log_mel.std() + 1e-9)\n\n\nclass MelAug(nn.Module):\n    def __init__(self):\n        super(MelAug, self).__init__()\n        self.transforms = nn.Sequential(\n            torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n            torchaudio.transforms.TimeMasking(time_mask_param=15)\n        )\n\n    def forward(self, melspec: torch.Tensor) -> torch.Tensor:\n        return self.transforms(melspec)\n\nclass WavAug(nn.Module):\n    def __init__(self):\n        super(WavAug, self).__init__()\n\n    def forward(self, wav):\n        gain = torch.rand((1,)).item()\n        fade_const = 20\n        fade = torch.randint(low=0, high=fade_const, size=(1,)).item()\n        transform = nn.Sequential(\n            #torchaudio.transforms.Resample(48000, SR),\n            torchaudio.transforms.Vol(gain),\n            torchaudio.transforms.Fade(fade, fade_const - fade)\n        )\n        return transform(wav)\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision.datasets import DatasetFolder\nfrom torch.utils.data import Dataset, ConcatDataset, Subset, DataLoader\nfrom torch.utils.data import WeightedRandomSampler\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence\nimport os\n\nclass CommonVoiceDataset(Dataset):\n    \n    def __init__(self, root, lblpath, transform=None):\n        super(CommonVoiceDataset).__init__()\n        self.root = root\n        self.targets = None\n        self.transform = None\n        meta = pd.read_csv(lblpath)\n        self.files = meta.filename.values\n        self.targets = meta.cleaned_text.values\n        if transform is not None:\n            self.transform = transform\n        \n        \n    def __getitem__(self, idx):\n        filepath = os.path.join(self.root, self.files[idx])\n        mp3, sr = torchaudio.load_wav(filepath)\n        if self.transform is not None:\n            mp3 = self.transform(mp3)\n        mp3 = mp3.squeeze()\n        target = [CHAR_VOCAB[c] for c in self.targets[idx].lower()]\n        n_frames = mp3.shape[0] // 256 + 1 # hop_length = 256\n        return mp3, n_frames // 2, torch.Tensor(target).type(torch.int), len(target)\n\n  \n\n    def __len__(self):\n        return len(self.files)\n\n\ndef collate_fn(batch):\n    X, X_lens, y, y_lens = zip(*batch)\n    X = pad_sequence(X, batch_first=True)\n    y = pad_sequence(y, batch_first=True)\n    return X, torch.Tensor(X_lens).type(torch.int32), y, torch.Tensor(y_lens).type(torch.int32)\n\n\ndef make_loader(root, lblpath, transform=None, bs=512, train=True):\n    dataset = CommonVoiceDataset(root, lblpath, transform)\n    meta = pd.read_csv(lblpath)\n    weights = torch.ones_like(torch.Tensor(meta.index), dtype=torch.float32)\n    if train:\n        weights = SR * 5 / meta['audio_len'].values\n    sampler = WeightedRandomSampler(weights, num_samples=len(weights))\n    loader = DataLoader(dataset, batch_size=bs, num_workers=0, pin_memory=True, \n                              collate_fn=collate_fn, drop_last=True, sampler=sampler)\n    return loader","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import editdistance\n\ndef wer(pred, lbl):\n    lbl_tok = lbl.split()\n    return editdistance.eval(pred.split(), lbl_tok) / len(lbl_tok) * 100\n\ndef cer(pred, lbl):\n    lbl = lbl.strip()\n    return editdistance.eval(pred, lbl) / len(lbl) * 100","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_text(pred, target):\n    pred_shifted = np.append(pred[1:], 0)\n    char_pred = pred[pred != pred_shifted]\n    text_pred = ''.join(ALPHABET[char_pred].squeeze().tolist())\n    target = target.squeeze().cpu().numpy()\n    text_target = ''.join(ALPHABET[target].tolist())\n    return text_pred, text_target\n    ","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score, classification_report\nfrom tqdm.notebook import tqdm\nfrom itertools import groupby\n\ndef remove_dups(text_list):\n    return [i[0] for i in groupby(text_list.cpu().detach())]\n\ndef train(epochs, model, optimizer, scheduler, device, early_stopping,\n          train_loader, valid_loader=None, grad_acum=1, criterion=nn.CTCLoss()):\n    process = nn.Sequential(\n          LogMelSpectrogram(SR, N_MELS).to(device),\n          MelAug().to(device)\n    )\n    clip = 15\n    val_table = wandb.Table(columns=[\"Epoch\", \"Predicted Text\", \"True Text\"])\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        tr_loss, val_loss = 0, 0\n        train_wer, train_cer = 0, 0\n        tr_steps, val_steps = 0, 0\n        for batch in tqdm(train_loader):\n            model.train()\n            train_input, input_lengths, target, target_lengths = batch\n            target = target.to(device, non_blocking=True)\n            input_lengths = input_lengths.to(device, non_blocking=True)\n            target_lengths = target_lengths.to(device, non_blocking=True)\n            X = process(train_input.to(device))\n            out = model(X).permute(2, 0, 1)\n            loss = criterion(out, target, input_lengths, target_lengths)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n            tr_loss += loss.item()\n            loss.backward()\n            wandb.log({'loss/train' : tr_loss / (tr_steps + 1)})\n            tr_steps += 1\n            if (tr_steps % grad_acum) == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n                \n            pred = torch.argmax(out, dim=2).squeeze().cpu().detach().numpy()\n            bs = pred.shape[1]\n            for pred_el, target_el in zip(pred.transpose(1, 0), target):\n                text_pred, text_target = to_text(pred_el.squeeze(), target_el)\n                \n                train_wer += wer(text_pred)\n                train_cer += cer(text_target)\n                \n        print(f'train wer: {train_wer / (len(train_loader) * bs)}, train_cer: {train_cer / (len(train_loader) * bs)}')\n        val_cer, val_wer = 0, 0\n        if valid_loader is not None:\n            for batch in tqdm(valid_loader):\n                model.eval()\n                with torch.no_grad():\n                    val_input, input_lengths, target, target_lengths = batch\n                    target = target.to(device, non_blocking=True)\n                    input_lengths = input_lengths.to(device, non_blocking=True)\n                    target_lengths = target_lengths.to(device, non_blocking=True)\n                    X = process(val_input.to(device))    \n                    out = model(X).permute(2, 0, 1)\n                    loss = criterion(out, target, input_lengths, target_lengths)\n\n                    val_loss += loss.item()\n                    pred = torch.argmax(out, dim=2).squeeze().cpu().detach().numpy()\n                    text_pred, text_target = to_text(pred, target)\n                    val_wer += wer(text_pred, text_target)\n                    val_cer += cer(text_pred, text_target)\n                    if val_steps < 5:\n                        val_table.add_data(epoch, text_pred, text_target)\n                        print(f'prediction: {text_pred}\\nlabel: {text_target}')        \n                    val_steps += 1\n                    wandb.log({'loss/val' : val_loss / (val_steps + 1)})\n            wandb.log({'wer/val' : val_wer / len(valid_loader)})\n            wandb.log({'cer/val' : val_cer / len(valid_loader)})\n            early_stopping(val_loss, model, epoch)\n            scheduler.step(val_loss)\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n    wandb.log({\"val examples\": val_table})","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import islice\n\nlr = 1e-2\nepochs = 10\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nwandb.init(project='dla hw2', name='CommonVoice weighted sampling')\nmodel = QuarzNet(config).to(device)\nwandb.watch(model)\noptimizer = torch_optimizer.NovoGrad(\n                        model.parameters(),\n                        lr=lr,\n                        betas=(0.8, 0.5),\n                        weight_decay=0.001,\n)\n\ncheckpoint = torch.load('../input/checkpoint/checkpoint (2)')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nscheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0, last_epoch=-1)\nscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\nearly_stopping = EarlyStopping(checkpoint='./checkpoint', patience=10, verbose=True)\nwav_aug = WavAug()\ntrain_loader = make_loader('../input/common-voice/cv-valid-train', '../input/checkpoint/cv-valid-train.csv',\n                           transform=wav_aug, bs=96, train=True)\ndev_loader = make_loader('../input/common-voice/cv-valid-dev', '../input/checkpoint/cv-valid-dev.csv',\n                         transform=wav_aug, bs=1, train=False)","execution_count":24,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marinaruck\u001b[0m (use `wandb login --relogin` to force relogin)\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.8 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Tracking run with wandb version 0.10.4<br/>\n                Syncing run <strong style=\"color:#cdcd00\">CommonVoice weighted sampling</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/arinaruck/dla%20hw2\" target=\"_blank\">https://wandb.ai/arinaruck/dla%20hw2</a><br/>\n                Run page: <a href=\"https://wandb.ai/arinaruck/dla%20hw2/runs/em2o5uvd\" target=\"_blank\">https://wandb.ai/arinaruck/dla%20hw2/runs/em2o5uvd</a><br/>\n                Run data is saved locally in <code>wandb/run-20201023_192614-em2o5uvd</code><br/><br/>\n            "},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nimport heapq\n\nALPHABET = np.array([''] + list(string.ascii_lowercase) + [' '])\n\ndef update_pred(pred, c):\n    if pred != '' and c == pred[-1]: \n        return pred\n    return pred + c\n  \ndef beam_search(probs, lm_model, beam_width=256, alpha=1, beta=1, gamma=0.1):\n    cache = {}\n    beam = {'' : 1.0}\n    probs = probs.squeeze()\n    for frame in probs:\n        curr_beam = defaultdict(float)\n        for prefix, prob in beam.items():\n            for c, p in enumerate(frame):\n                pred = update_pred(prefix, ALPHABET[c])\n                curr_beam[pred] +=  prob * (alpha * p.item() + beta * get_lm_prob(lm_model, pred) + gamma * len(pred))\n        beam_items = heapq.nlargest(beam_width, list(curr_beam.items()), key=lambda x: x[1])\n        beam = {k: v for k, v in beam_items}\n            \n        best_pred, best_prob = heapq.nlargest(1, beam.items(), key=lambda x: x[1])[0]\n    return best_pred.strip()","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataset = CommonVoiceDataset('../input/common-voice/cv-valid-dev', '../input/checkpoint/cv-valid-dev.csv')\nval_subset = Subset(val_dataset, list(range(32)))\nsubset_loader = DataLoader(val_subset, batch_size=1, num_workers=0, pin_memory=True, \n                            collate_fn=collate_fn, drop_last=True)\n\nprocess = nn.Sequential(\n        LogMelSpectrogram(SR, N_MELS).to(device)\n)\n\ntable = wandb.Table(columns=[\"Argmax Text\", \"Beam search Text\", \"True Text\"])        \nval_cer, val_wer = 0, 0\nval_steps = 0\nfor batch in tqdm(subset_loader):\n    model.eval()\n    with torch.no_grad():\n        val_input, input_lengths, target, target_lengths = batch\n        target = target.to(device, non_blocking=True)\n        input_lengths = input_lengths.to(device, non_blocking=True)\n        target_lengths = target_lengths.to(device, non_blocking=True)\n        X = process(val_input.to(device))    \n        out = model(X).permute(2, 0, 1)\n        pred = torch.argmax(out, dim=2).squeeze().cpu().detach().numpy()\n        text_pred, text_target = to_text(pred, target)\n        text_beam = beam_search(torch.exp(out), None, beam_width=64, alpha=1, beta=0, gamma=0)\n        val_wer += wer(text_beam, text_target)\n        val_cer += cer(text_beam, text_target)\n        if val_steps < 5:\n            table.add_data(text_pred, text_beam, text_target)\n            print(f'argmax prediction: {text_pred}\\nbeam seach prediction: {text_beam}\\nlabel: {text_target}')   \n        val_steps += 1\nprint(f'wer: {val_wer / val_steps}, cer: {val_cer / val_steps}')\nwandb.log({\"examples\": table})","execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c40ad87bcdc45d98581b9981dd6908b"}},"metadata":{}},{"output_type":"stream","text":"argmax prediction: be careful lith you pronostications had the stranger\nbeam seach prediction: be careful lith you prognostications had the stranger\nlabel: be careful with your prognostications said the stranger\nargmax prediction: anfie sheud ut u ee the plize o efleee un\nbeam seach prediction: ined fie sheud ut u e the plize o efle une\nlabel: then why should they be surprised when they see one\nargmax prediction: a oung arab waked the ledde own ad package anded and geited the englishman\nbeam seach prediction: a oung arab waked the leade downd ad package anded and geieted the englishman\nlabel: a young arab also loaded down with baggage entered and greeted the englishman\nargmax prediction: i felhtd that everything thi owoed would ee de twroeeed\nbeam seach prediction: i felht that everything thi owed would e de twroed\nlabel: i thought that everything i owned would be destroyed\nargmax prediction: he woved abant invisible but every one could hear him\nbeam seach prediction: he woved aband invisible but every one could hear him\nlabel: he moved about invisible but everyone could hear him\n\nwer: 65.99225427350429, cer: 30.117012329957685\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataset = CommonVoiceDataset('../input/common-voice/cv-valid-dev', '../input/checkpoint/cv-valid-dev.csv')\nval_subset = Subset(val_dataset, list(range(3)))\nsubset_loader = DataLoader(val_subset, batch_size=1, num_workers=0, pin_memory=True, \n                            collate_fn=collate_fn, drop_last=True)\n\nprocess = nn.Sequential(\n        LogMelSpectrogram(SR, N_MELS).to(device)\n)\n\ntable = wandb.Table(columns=[\"Argmax Text\", \"Beam search Text\", \"True Text\"])        \nval_cer, val_wer = 0, 0\nval_steps = 0\nfor batch in tqdm(subset_loader):\n    model.eval()\n    with torch.no_grad():\n        val_input, input_lengths, target, target_lengths = batch\n        target = target.to(device, non_blocking=True)\n        input_lengths = input_lengths.to(device, non_blocking=True)\n        target_lengths = target_lengths.to(device, non_blocking=True)\n        X = process(val_input.to(device))    \n        out = model(X).permute(2, 0, 1)\n        pred = torch.argmax(out, dim=2).squeeze().cpu().detach().numpy()\n        text_pred, text_target = to_text(pred, target)\n        text_beam = beam_search(torch.exp(out), loaded, beam_width=16, alpha=1, beta=0.5, gamma=1e-3)\n        val_wer += wer(text_beam, text_target)\n        val_cer += cer(text_beam, text_target)\n        if val_steps < 5:\n            table.add_data(text_pred, text_beam, text_target)\n            print(f'argmax prediction: {text_pred}\\nbeam seach + lm prediction: {text_beam}\\nlabel: {text_target}')   \n        val_steps += 1\nprint(f'wer: {val_wer / val_steps}, cer: {val_cer / val_steps}')\nwandb.log({\"examples\": table})","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b238b35da64dd393f4c9277296bc0c"}},"metadata":{}},{"output_type":"stream","text":"argmax prediction: be careful lith you pronostications had the stranger\nbeam seach + lm prediction: tions had the stranger\nlabel: be careful with your prognostications said the stranger\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(epochs, model, optimizer, scheduler, device, early_stopping, train_loader, dev_loader)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
